{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab2.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:41.748397Z",
     "iopub.status.busy": "2025-02-18T17:53:41.748237Z",
     "iopub.status.idle": "2025-02-18T17:53:42.997919Z",
     "shell.execute_reply": "2025-02-18T17:53:42.997395Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from resources.hashutils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from resources.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 2 <br>Solving optimization problems with <br> Gradient Descent and <br> Stochastic Gradient Descent </center></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "**Part I**:  Estimating $\\lambda$ for an exponential distribution\n",
    "+ I-1) Sampling a dataset\n",
    "+ I-2) Cost function\n",
    "+ I-3) Analytical solution\n",
    "\t+ I-3.1) Compute $\\lambda_*$\n",
    "\t+ I-3.2) Plot\n",
    "\t+ I-3.3) Variations due to uncertainty in the data\n",
    "\t\t+ I-3.3.1) Variations in $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda_*$\n",
    "\t\t+ I-3.3.2) Plot $\\mathcal{J}_1(\\lambda;\\mathcal{D})$\n",
    "\t\t+ I-3.3.3) Histogram of $\\lambda_*$\n",
    "+ I-4) Gradient descent\n",
    "\t+ I-4.1) Compute $\\mathcal{J}_1'(\\lambda,\\mathcal{D})$\n",
    "\t+ I-4.2) Plot $\\mathcal{J}_1'$\n",
    "\t+ I-4.3) Code gradient descent\n",
    "\t+ I-4.4) Plot gradient descent\n",
    "\t+ I-4.5) Count evaluations of $\\mathcal{J}_1'(\\lambda;\\mathcal{D})$\n",
    "\n",
    "**Part II**: Estimating $\\mu$ and $\\sigma^2$ for a normal distribution\n",
    " \n",
    "+ II-1) Cost function\n",
    "+ II-2) Analytical solution\n",
    "\t+ II-2.1) Compute $\\mu_*$ and $\\sigma^2_*$\n",
    "+ II-3) Gradient descent\n",
    "\t+ II-3.1) Compute the gradient\n",
    "\t+ II-3.2) 2D gradient descent\n",
    "\t+ II-3.3) Plot gradient descent\n",
    "\t+ II-3.4) Run this GD over a grid of initial conditions.\n",
    "+ II-4) Stochastic gradient descent\n",
    "\t+ II-4.1) Code stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this lab exercise we use gradient descent (GD) and stochastic gradient descent (SGD) to solve the maximum likelihood problem, which is an important problem in statistics. The details of maximum likelihood will be covered in the lecture on point estimation. Here we are only concerned with the numerical solution of the optimzation problem -- not its formulation or purpose.\n",
    "\n",
    "# Problem statement\n",
    "\n",
    "Suppose that we have a system that produces real-valued (scalar) measurements. The system is modeled as a random variable $Y$ with an unknown pdf $p_Y$. The dataset $\\mathcal{D}$ consists of $N$ measurements iid sampled from $Y$.\n",
    "\\begin{equation*}\n",
    "\\mathcal{D} = \\{ y_i \\}_N \\sim Y\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal is to use $\\mathcal{D}$ to estimate a distribution $p$ that best approximates $p_Y$. Here we use the *maximum likelihood* technique, which solves the problem in two steps. The first step is to guess which family of distributions best fits $Y$. This can be done by looking at a histogram of the data, or based on expert opinion. \n",
    "The second step, given our choice, is to solve an optimization problem for the optimal values of the parameters. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\underset{\\theta_1,...\\theta_D}{\\text{minimize}}\\;  -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(y_i;\\theta_1,...\\theta_D)  \n",
    "\\end{equation*}\n",
    "\n",
    "Here, $p(y_i;\\theta_1,...\\theta_D)$ is the pdf of the selected family with parameters $\\theta_1,...,\\theta_D$, evaluated on the sample $y_i\\in\\mathcal{D}$. It is not important for this lab to understand the reasoning behind this formula. For now we regard it simply as a function to be minimized. We refer to this function as the *cost function* $\\mathcal{J}(\\theta_1,...\\theta_D;\\mathcal{D})$. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}(\\theta_1,...\\theta_D;\\mathcal{D}) = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(y_i;\\theta_1,...\\theta_D)  \n",
    "\\end{equation*}\n",
    "\n",
    "This notation emphasize that we minimize $\\mathcal{J}$ over the parameters $\\theta_1,...,\\theta_D$ with the dataset $\\mathcal{D}$ held fixed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part I:**  Estimating $\\lambda$ for an exponential distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-1) Sampling a dataset\n",
    "\n",
    "The method `draw_50_samples_from_I` returns an array of 50 floating point values that represent 50 iid measurements from a system. We pretend that we do not know the underlying process, but in fact the data was generated from an exponential distribution with $\\lambda=10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.000195Z",
     "iopub.status.busy": "2025-02-18T17:53:42.999917Z",
     "iopub.status.idle": "2025-02-18T17:53:43.003551Z",
     "shell.execute_reply": "2025-02-18T17:53:43.002918Z"
    }
   },
   "outputs": [],
   "source": [
    "D = draw_50_samples_from_I()\n",
    "N = len(D)\n",
    "\n",
    "print(f\"The dataset has {N} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data. The top row in the figure below shows the data points arranged on a line. The bottom row shows a histogram of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.051790Z",
     "iopub.status.busy": "2025-02-18T17:53:43.051562Z",
     "iopub.status.idle": "2025-02-18T17:53:43.330591Z",
     "shell.execute_reply": "2025-02-18T17:53:43.330094Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5),nrows=2,sharex=True)\n",
    "ax[0].plot(D,np.zeros(N),'r.')\n",
    "ax[1].hist(D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram, and perhaps based on our understanding of the system, we postulate that $Y$ is exponentially distributed. \n",
    "\n",
    "\\begin{equation*}\n",
    "p(y;\\lambda) = \\lambda e^{-\\lambda y}\n",
    "\\end{equation*}\n",
    "$\\lambda$ is unknown. We plug this formula into the expression for the cost function:\n",
    "\\begin{align*}\n",
    "\\mathcal{J}_1(\\lambda;\\mathcal{D}) &= -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(y_i;\\lambda)   \\\\\n",
    " &= -\\frac{1}{N}\\sum_{i=1}^{N} \\ln\\left( \\lambda e^{-\\lambda y} \\right)  \\\\\n",
    " &= -\\frac{1}{N}\\sum_{i=1}^{N}\\left( \\ln(\\lambda) -\\lambda y \\right)  \\\\\n",
    "&=  - \\ln\\lambda +  \\frac{\\lambda}{N}\\sum_{i=1}^N y_i\n",
    "\\end{align*}\n",
    "We call this $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ (with subindex 1) to distinguish it from the cost function in part 2 of this lab. \n",
    "The expression is simplified by defining $\\bar{y}$ as the mean of the samples in $\\mathcal{D}$.\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_1(\\lambda;\\mathcal{D}) =  - \\ln\\lambda + \\lambda \\: \\bar{y}\n",
    "\\end{equation*}\n",
    "\n",
    "Next we will write code to find the value of $\\lambda_*$ that minimizes $\\mathcal{J}_1(\\lambda;\\mathcal{D})$. We will do this by both analytical and numerical means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## I-2) Cost function\n",
    "\n",
    "Write a function that computes $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ for a given $\\lambda$ and dataset $\\mathcal{D}$. (This can be done with one line of code.)\n",
    "\n",
    "**Hint**: NumPy aggregations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.332565Z",
     "iopub.status.busy": "2025-02-18T17:53:43.332386Z",
     "iopub.status.idle": "2025-02-18T17:53:43.336102Z",
     "shell.execute_reply": "2025-02-18T17:53:43.335680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curlyJ1(lmbda,D):\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-3) Exact solution\n",
    "\n",
    "The optimality conditions for this problem tell us that, if a solution $\\lambda^*$ exists, then it must be a *stationary point* of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$, meaning one where the derivative is zero. That is, if $\\lambda^*$ is a solution, then\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_1'(\\lambda^*;\\mathcal{D}) = 0\n",
    "\\end{equation*}\n",
    "This does not mean that all stationary points are solutions. Some stationary points may be maxima instead of minima. Some may be local minima but not global minima. But if there is a small number of stationary points, then we can evaluate $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ on each one, and be assured that the one with the lowest value is the global solution. \n",
    "\n",
    "Our strategy for finding $\\lambda^*$ therefore begins with finding all of the stationary points. That is, all of the values $\\lambda_s$ that satisfy $\\mathcal{J}_1'(\\lambda_s,\\mathcal{D}) = 0$. Solving this equation produces a *unique* stationary point:\n",
    "\\begin{equation*}\n",
    "\\lambda_s = 1/\\bar{y}\n",
    "\\end{equation*}\n",
    "\n",
    "Since there is only one, we conclude that it is $\\lambda^*$.\n",
    "\\begin{equation*}\n",
    "\\lambda^* = 1/\\bar{y}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-3.1) Compute $\\lambda^*$\n",
    "\n",
    "Write a function that computes $\\lambda^*$ for a given $\\mathcal{D}$. (This can be done in one line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.347194Z",
     "iopub.status.busy": "2025-02-18T17:53:43.347035Z",
     "iopub.status.idle": "2025-02-18T17:53:43.350193Z",
     "shell.execute_reply": "2025-02-18T17:53:43.349824Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_lambda_star(D):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-3.2) Plot\n",
    "\n",
    "Plot the cost function $\\mathcal{J}_1$ evaluated on logarithmically spaced points (NumPy's `logspace`) ranging from $10^{-2}$ to $10^{-0.5}$. Place a vertical line (matplotlib `axvline`) at $\\lambda_*$.\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"resources/lambdastar.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.356576Z",
     "iopub.status.busy": "2025-02-18T17:53:43.356414Z",
     "iopub.status.idle": "2025-02-18T17:53:43.675888Z",
     "shell.execute_reply": "2025-02-18T17:53:43.675333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas = ...   # TODO\n",
    "J1s = ...  # TODO\n",
    "...\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "ax = plt.subplot()\n",
    "ax.plot(...,...,'.-')  # TODO\n",
    "ax.axvline(...,color='r',linestyle='--')  # TODO\n",
    "ax.grid(linestyle=':')\n",
    "ax.set_xlabel('$\\\\lambda$',fontsize=16)\n",
    "ax.set_ylabel('$\\\\mathcal{J}_1(\\\\lambda)$',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3.3) Variations due to uncertainty in the data\n",
    "\n",
    "The computation of both the cost function $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and the optimal value $\\lambda^*$ were based on the randomly sampled dataset $\\mathcal{D}$. Next we will see the effect that variations in $\\mathcal{D}$ have on $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### I-3.3.1) Variations in $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda^*$\n",
    "\n",
    "Compute $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda^*$ for 300 independently samples datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.677836Z",
     "iopub.status.busy": "2025-02-18T17:53:43.677659Z",
     "iopub.status.idle": "2025-02-18T17:53:43.753386Z",
     "shell.execute_reply": "2025-02-18T17:53:43.752932Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_datasets = 300\n",
    "Ds = draw_50_samples_from_I_300_times()\n",
    "J1_samples = np.empty((num_datasets,lmbdas.shape[0]))\n",
    "lambda_star_samples = np.empty(num_datasets)\n",
    "for i in range(num_datasets):\n",
    "    D = Ds[i,:]\n",
    "    J1_samples[i,:] = ...  # TODO\n",
    "    lambda_star_samples[i] = ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_3_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### I-3.3.2) Plot $\\mathcal{J}_1(\\lambda;\\mathcal{D})$\n",
    "\n",
    "Plot the 300 versions of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ on a single plot. Pass parameters `c='k'`,`linewidth=0.5`, and `alpha=0.1` to the `plot` function.\n",
    "\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"resources/lambdastar_stoch.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:43.767025Z",
     "iopub.status.busy": "2025-02-18T17:53:43.766835Z",
     "iopub.status.idle": "2025-02-18T17:53:44.054958Z",
     "shell.execute_reply": "2025-02-18T17:53:44.054445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,3))\n",
    "ax = plt.subplot()\n",
    "ax.plot(...)  # TODO\n",
    "ax.grid(linestyle=':')\n",
    "ax.set_xlabel('$\\\\lambda$',fontsize=16)\n",
    "ax.set_ylabel('$\\\\mathcal{J}_1(\\\\lambda)$',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### I-3.3.3) Histogram of $\\lambda^*$\n",
    "\n",
    "Make a histogram of the 300 samples of $\\lambda^*$. The histogram should have 50 bins. Draw a vertical line on it indicating the average of the values.\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"resources/lambdastar_hist.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.056917Z",
     "iopub.status.busy": "2025-02-18T17:53:44.056700Z",
     "iopub.status.idle": "2025-02-18T17:53:44.218789Z",
     "shell.execute_reply": "2025-02-18T17:53:44.218354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.subplot()\n",
    "ax.hist(...)  # TODO\n",
    "ax.axvline(...)  # TODO\n",
    "ax.set_xlabel('$\\\\lambda^*$',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving optimization problems is usually not this easy to do by hand. More often we must use a numerical method executed on a computer. There are many algorithms for doing this, and they divide roughly into two types: ones that use gradient information and ones that do not. Here we will demonstrate canonical examples of each of these types. \n",
    "\n",
    "## I-4 Solution with gradient descent\n",
    "\n",
    "Gradient-based algorithms assume that we have at our disposal a method for evaluating the derivative (a.k.a. the gradient) of the cost function. Given this requirement, the procedure begins by making a guess $\\lambda_0$. We call $\\lambda_0$ the *initial condition* of the algorithm. If $\\lambda_0$ happens to be a stationary point (ie. $\\mathcal{J}_1'(\\lambda_0;\\mathcal{D})=0$), then the procedure terminates and returns $\\lambda_0$. Otherwise it proceeds to compute $\\lambda_1$ with\n",
    "\\begin{equation*}\n",
    "\\lambda_{1} = \\lambda_{0} - \\gamma \\: \\mathcal{J}_1'(\\lambda_0;\\mathcal{D}) \n",
    "\\end{equation*}\n",
    "Here $\\gamma$ is a positive number called the *step size*. The reasoning behind this formula is that by taking steps in the direction of the *negative gradient*, the algorithm will eventually reach a local minimum. This is true so long as $\\gamma$ is chosen correctly (not too big). The general formula for gradient descent is,\n",
    "\\begin{equation*}\n",
    "\\lambda_{k+1} = \\lambda_{k} - \\gamma \\: \\mathcal{J}_1'(\\lambda_k;\\mathcal{D}) \\qquad k\\in\\{0,...,K-1\\}\n",
    "\\end{equation*}\n",
    "Here $K$ is the number of steps taken.\n",
    "There are a couple enhancements to consider. \n",
    "\n",
    "1) We can stop the process once we are within a tolerance value of a stationary point. That is, if the gradient becomes less than a pre-determined value $\\tau$:\n",
    "\\begin{equation*}\n",
    "|\\mathcal{J}_1'(\\lambda_k;\\mathcal{D})|<\\tau\n",
    "\\end{equation*}\n",
    "which is equivalent to \n",
    "\\begin{equation*}\n",
    "|\\lambda_{k+1}-\\lambda_k|<\\gamma\\tau\n",
    "\\end{equation*}\n",
    "\n",
    "2) For our particular problem, it is important to ensure that $\\lambda_{k+1}$ does not become negative, since $\\mathcal{J}_1'(\\lambda_{k+1};\\mathcal{D})$ would not be defined. To prevent this we can use $\\lambda_{k+1} =\\lambda_k/2$ whenever the standard formula produces a negative value.\n",
    "\n",
    "In this lab exercise we will implement the simple version of gradient descent *without these two enhancements*. \n",
    "\n",
    "We will need a function that computes the gradient of $\\mathcal{J}_1$. \n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_1'(\\lambda;\\mathcal{D}) =  - \\frac{1}{\\lambda} +  \\bar{y}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-4.1) Compute $\\mathcal{J}_1'(\\lambda,\\mathcal{D})$\n",
    "\n",
    "Write a function that computes $\\mathcal{J}_1'(\\lambda,\\mathcal{D})$ for a given $\\lambda$ and $\\mathcal{D}$. (This can be done with one line of code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.220692Z",
     "iopub.status.busy": "2025-02-18T17:53:44.220522Z",
     "iopub.status.idle": "2025-02-18T17:53:44.224403Z",
     "shell.execute_reply": "2025-02-18T17:53:44.224000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curlyJ1prime(lmbda,D):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-4.2) Plot $\\mathcal{J}_1'$\n",
    "\n",
    "Plot the gradient $\\mathcal{J}'_1$ evaluated on logarithmically spaced points (numpy `logspace`) ranging from $10^{-2}$ to $10^{-0.5}$. Place a vertical line (matplotlib `axvline`) at $\\lambda^*$.\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"resources/jprime.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.235952Z",
     "iopub.status.busy": "2025-02-18T17:53:44.235802Z",
     "iopub.status.idle": "2025-02-18T17:53:44.368291Z",
     "shell.execute_reply": "2025-02-18T17:53:44.367406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas = np.logspace(-2,-0.5)\n",
    "J1ps = ...  # TODO\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "ax = plt.subplot()\n",
    "ax.plot(...,...,'.-')  # TODO\n",
    "ax.axvline(...,color='r',linestyle='--')  # TODO\n",
    "ax.grid(linestyle=':')\n",
    "ax.set_xlabel('$\\\\lambda$',fontsize=16)\n",
    "ax.set_ylabel(\"$\\\\mathcal{J}'_1(\\\\lambda)$\",fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-4.3) Code gradient descent\n",
    "\n",
    "Write a function called `GD_1D` that takes arguments \n",
    "+ `Jprime` ... a function that evaluates the derivative of the cost function. Example: `GD_1D(curlyJprime,D)`\n",
    "+ `D` ... the dataset\n",
    "+ `lambda_init` ... the initial condition for gradient descent\n",
    "\n",
    "Use $\\gamma=0.01$ and $K=10$.\n",
    "The function should return a numpy array of length $K$, with `lambda_init` in the zeroth position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.370486Z",
     "iopub.status.busy": "2025-02-18T17:53:44.370159Z",
     "iopub.status.idle": "2025-02-18T17:53:44.374244Z",
     "shell.execute_reply": "2025-02-18T17:53:44.373793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GD_1D(Jprime,D,lmbda_init=0.3):\n",
    "    gamma=0.01\n",
    "    K=10\n",
    "    lmbdas = np.empty(K)\n",
    "    lmbdas[0] = ... # TODO\n",
    "    ...  # TODO (a for loop)\n",
    "    return lmbdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.375808Z",
     "iopub.status.busy": "2025-02-18T17:53:44.375638Z",
     "iopub.status.idle": "2025-02-18T17:53:44.378179Z",
     "shell.execute_reply": "2025-02-18T17:53:44.377797Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas_gd = GD_1D(curlyJ1prime,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_4_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-4.4) Plot gradient descent\n",
    "\n",
    "Create a plot showing the convergence of $\\{\\lambda_k\\}_K$ (from gradient descent) to $\\lambda_*$\n",
    "\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"resources/lambdaconv.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.390981Z",
     "iopub.status.busy": "2025-02-18T17:53:44.390582Z",
     "iopub.status.idle": "2025-02-18T17:53:44.503417Z",
     "shell.execute_reply": "2025-02-18T17:53:44.502841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas_gd = GD_1D(curlyJ1prime,D)\n",
    "K_gd = len(lmbdas_gd)\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "ax = plt.subplot()\n",
    "ax.plot(...,...,'-o')  # TODO\n",
    "ax.hlines(eval_lambda_star(D),0,K_gd,color='r',linestyles=':')\n",
    "ax.set_xlabel('$k$',fontsize=16)\n",
    "ax.set_ylabel('$\\\\lambda_k$',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I-4.5) Count evaluations of $\\mathcal{J}_1'(\\lambda;\\mathcal{D})$\n",
    "\n",
    "\n",
    "**Hint**: `np.where`\n",
    "\n",
    "How many evaluations of $\\mathcal{J}_1'(\\lambda;\\mathcal{D})$ were needed for gradient descent to reach within 0.01 of $\\lambda^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.505333Z",
     "iopub.status.busy": "2025-02-18T17:53:44.505181Z",
     "iopub.status.idle": "2025-02-18T17:53:44.509095Z",
     "shell.execute_reply": "2025-02-18T17:53:44.508725Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_J1prime_evals(lmbdas_gd,lambda_star):\n",
    "    ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_4_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part II:** Estimating $\\mu$ and $\\sigma^2$ for a normal distribution\n",
    "\n",
    "---\n",
    "\n",
    "We now repeat the exercise with a different system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.515791Z",
     "iopub.status.busy": "2025-02-18T17:53:44.515646Z",
     "iopub.status.idle": "2025-02-18T17:53:44.518936Z",
     "shell.execute_reply": "2025-02-18T17:53:44.518580Z"
    }
   },
   "outputs": [],
   "source": [
    "D = draw_50_samples_from_II()\n",
    "N = D.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a scatter plot and histogram of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.520527Z",
     "iopub.status.busy": "2025-02-18T17:53:44.520382Z",
     "iopub.status.idle": "2025-02-18T17:53:44.746695Z",
     "shell.execute_reply": "2025-02-18T17:53:44.746126Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5),nrows=2,sharex=True)\n",
    "ax[0].plot(D,np.zeros(N),'r.')\n",
    "ax[1].hist(D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot clearly shows that the data cannot be exponentially distributed since it includes negative value. We decide too use a Gaussian distribution.  The pdf for the Gaussian distribution is:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{(y-\\mu)^2}{2\\sigma^2}   \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Plugging this into the maximum likelihood cost function and doing some simple algebra we obtain:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_2(\\mu,\\sigma^2;\\mathcal{D}) = \n",
    "\\frac{1}{2}\\ln(2\\pi\\sigma^2)\n",
    "+\\frac{1}{2N\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2 \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "## II-1) Cost function\n",
    "\n",
    "Write a function that computes $\\mathcal{J}_2(\\mu,\\sigma^2;\\mathcal{D})$ for given $\\mu$, $\\sigma^2$, and dataset $\\mathcal{D}$. (This can be done in one line of code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.748546Z",
     "iopub.status.busy": "2025-02-18T17:53:44.748390Z",
     "iopub.status.idle": "2025-02-18T17:53:44.752066Z",
     "shell.execute_reply": "2025-02-18T17:53:44.751728Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curlyJ2(mu,sigma2,D):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-2) Exact solution\n",
    "\n",
    "We can use algebra to find a solution, just as we did with the exponential distribution, by equating the derivative of the cost function to zero. Now however, because our search space is two-dimensional $(\\mu,\\sigma^2)$, we must work with a vector *gradient* instead of a scalar *derivative*. \n",
    "\n",
    "You can verify that the partial derivatives of $\\mathcal{J}_2(\\mu,\\sigma^2;\\mathcal{D})$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu} &= \\frac{\\mu-\\bar{y}}{\\sigma^2} \\\\ \n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2} &= \\frac{1}{2\\sigma^2} \\left(1 -\t\n",
    "\\frac{1}{N\\sigma^2}\\sum_{i=1}^N \\left( y_i-\\mu  \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Equating $\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu}$ and $\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2}$ to zero we get two equations for stationary $\\mu$ and $\\sigma^2$. From these we can find the *unique* stationary point, which is also the solution to the optimization problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_* &= \\bar{y} \\\\\n",
    "\\sigma^2_* &= \\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\bar{y})^2\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II-2.1) Compute $\\mu_*$ and $\\sigma^2_*$\n",
    "\n",
    "Write a function that computes $\\mu_*$ and $\\sigma^2_*$ for a given $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.763944Z",
     "iopub.status.busy": "2025-02-18T17:53:44.763795Z",
     "iopub.status.idle": "2025-02-18T17:53:44.767011Z",
     "shell.execute_reply": "2025-02-18T17:53:44.766656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_mu_sigma2_star(D):\n",
    "    mu_star = ...   # TODO\n",
    "    sigma2_star = ...   # TODO\n",
    "    return mu_star, sigma2_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.768442Z",
     "iopub.status.busy": "2025-02-18T17:53:44.768295Z",
     "iopub.status.idle": "2025-02-18T17:53:44.770547Z",
     "shell.execute_reply": "2025-02-18T17:53:44.770202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_star, sigma2_star = eval_mu_sigma2_star(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-3) Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II-3.1) Compute the gradient\n",
    "\n",
    "Write a function that computes the gradient $\\left(\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu},\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2}\\right)$ as a function of $\\mu$, $\\sigma^2$, and $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.782769Z",
     "iopub.status.busy": "2025-02-18T17:53:44.782326Z",
     "iopub.status.idle": "2025-02-18T17:53:44.786408Z",
     "shell.execute_reply": "2025-02-18T17:53:44.785934Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient(mu,sigma2,D):\n",
    "    N = ...   # TODO\n",
    "    partialMu = ...   # TODO\n",
    "    partialSigma2 = ...   # TODO\n",
    "    return partialMu, partialSigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.788050Z",
     "iopub.status.busy": "2025-02-18T17:53:44.787879Z",
     "iopub.status.idle": "2025-02-18T17:53:44.790388Z",
     "shell.execute_reply": "2025-02-18T17:53:44.789999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dmu,dsigma2 = gradient(2,2,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II-3.2) 2D gradient descent\n",
    "\n",
    "Write the gradient descent algorithm for this 2D case. The code should be very similar to the 1D function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.802198Z",
     "iopub.status.busy": "2025-02-18T17:53:44.802040Z",
     "iopub.status.idle": "2025-02-18T17:53:44.806575Z",
     "shell.execute_reply": "2025-02-18T17:53:44.806186Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GD_2D(GRAD,D,mu_init,sigma2_init,num_epochs):\n",
    "    gamma_mu, gamma_sigma2 = 1.5, 20\n",
    "    N = D.shape[0]\n",
    "    numsteps = num_epochs*N   # each epoch is a sweep though N samples in the dataset\n",
    "    mu = np.empty(numsteps)\n",
    "    mu[0] = ...   # TODO\n",
    "    sigma2 = np.empty(numsteps)\n",
    "    sigma2[0] = ...   # TODO\n",
    "    for k in range(...):   # TODO\n",
    "        delmu, delsigma2 = GRAD(mu[k],sigma2[k],D)\n",
    "        mu[k+1] = ...   # TODO\n",
    "        sigma2[k+1] = ...   # TODO\n",
    "    return mu, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.808088Z",
     "iopub.status.busy": "2025-02-18T17:53:44.807917Z",
     "iopub.status.idle": "2025-02-18T17:53:44.811465Z",
     "shell.execute_reply": "2025-02-18T17:53:44.811099Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_gd, sigma2_gd = GD_2D(gradient,D,mu_init=0.5,sigma2_init=10,num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II-3.3) Plot gradient descent\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"resources/gd.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.832599Z",
     "iopub.status.busy": "2025-02-18T17:53:44.832426Z",
     "iopub.status.idle": "2025-02-18T17:53:44.938224Z",
     "shell.execute_reply": "2025-02-18T17:53:44.937657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_gd, sigma2_gd = GD_2D(gradient,D,mu_init=0.5,sigma2_init=10,num_epochs=2)\n",
    "mu_star, sigma2_star = eval_mu_sigma2_star(D)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(...,...,'.-')   # TODO\n",
    "ax.plot(...,...,'r*',markersize=16)   # TODO\n",
    "ax.set_xlabel('$\\\\mu_k$',fontsize=16)\n",
    "ax.set_ylabel('$\\\\sigma^2_k$',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3.4) Run this GD over a grid of initial conditions.\n",
    "\n",
    "The next cell runs gradient descent over a grid of initial conditions in the $(\\mu,\\sigma^2)$ plane. Notice that in every case the trajectory converges toward the true solution of the optimization problem $(\\mu_*,\\sigma^2_*)$, shown as a red star in the first plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:44.940197Z",
     "iopub.status.busy": "2025-02-18T17:53:44.940002Z",
     "iopub.status.idle": "2025-02-18T17:53:45.508148Z",
     "shell.execute_reply": "2025-02-18T17:53:45.507649Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_star, sigma2_star = eval_mu_sigma2_star(D)\n",
    "\n",
    "mu_range = np.linspace(0.1,2,10)\n",
    "sigma2_range = np.arange(15,25)\n",
    "\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('$\\\\mu_k$',fontsize=16)\n",
    "ax1.set_ylabel('$\\\\sigma^2_k$',fontsize=16)\n",
    "\n",
    "_, ax2 = plt.subplots(nrows=2,sharex=True)\n",
    "ax2[0].set_ylabel('$\\\\mu_k$',fontsize=16)\n",
    "\n",
    "\n",
    "ax2[1].set_ylabel('$\\\\sigma^2_k$',fontsize=16)\n",
    "ax2[1].set_xlabel('$k$',fontsize=16)\n",
    "\n",
    "for i, mu_init in enumerate(mu_range):\n",
    "    for j, sigma2_init in enumerate(sigma2_range):\n",
    "        mu_gd, sigma2_gd = GD_2D(gradient,D,mu_init=mu_init,sigma2_init=sigma2_init,num_epochs=2)\n",
    "        ax1.plot(mu_gd,sigma2_gd)\n",
    "        ax2[0].plot(mu_gd)\n",
    "        ax2[1].plot(sigma2_gd)\n",
    "\n",
    "ax1.plot(mu_star,sigma2_star,'r*',markersize=16)\n",
    "ax2[0].axhline(mu_star,0,100,color='r',linestyle='--')\n",
    "ax2[1].axhline(sigma2_star,0,100,color='r',linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-4) Stochastic gradient descent\n",
    "\n",
    "Recall the formula for the partial derivatives of $\\mathcal{J}_2$. It can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu} &= \\frac{1}{\\sigma^2} \\left( \\mu - \\frac{1}{N}\\sum_{y_i\\in\\mathcal{D}} y_i \\right) \\\\ \n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2} &= \\frac{1}{2\\sigma^2} \\left(1 -\t\n",
    "\\frac{1}{\\sigma^2}\\frac{1}{N}\\sum_{y_i\\in\\mathcal{D}} \\left( y_i-\\mu  \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that both of these involve a sum over the dataset $\\mathcal{D}$. While $\\sum_{y_i\\in\\mathcal{D}} y_i$ can be pre-computed, $\\sum_{y_i\\in\\mathcal{D}} \\left( y_i-\\mu  \\right)^2$ is inextricable linked to $\\mu$, and must therefore be re-computed at each step of gradient descent. This can be very time-consuming when $N$ is large. \n",
    "\n",
    "Stochastic gradient descent is the simple idea of approximating the gradient based on a sub-sample of the dataset $\\mathcal{D}$, which we call a **batch** $\\mathcal{B}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu} &\\approx \\frac{1}{\\sigma^2} \\left( \\mu - \\frac{1}{|\\mathcal{B}|}\\sum_{y_i\\in\\mathcal{B}} y_i \\right) \\\\ \n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2} &\\approx \\frac{1}{2\\sigma^2} \\left(1 -\t\n",
    "\\frac{1}{\\sigma^2}\\frac{1}{|\\mathcal{B}|}\\sum_{y_i\\in\\mathcal{B}} \\left( y_i-\\mu  \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "Here $|\\mathcal{B}|$ is the number of samples in the batch.\n",
    "Batches are obtained by splitting the dataset into equal parts. We will test SGD with batches of size 5. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II-4.1) Code stochastic gradient descent\n",
    "\n",
    "Next we will code stochastic gradient descent. The code is very similar to `GD_2D` with a few exceptions to note:\n",
    "\n",
    "1) It should begin by splitting $\\mathcal{D}$ into batches of size `batch_size`. The code below uses the `reshape` method to do this. Understand how it works, and then you can copy it directly into your `SGD` method.\n",
    "\n",
    "``` python\n",
    "N = D.shape[0]\n",
    "num_batches = int(N/batch_size)\n",
    "batches = D.reshape((num_batches,batch_size))\n",
    "```\n",
    "\n",
    "2) Each step in the iteration is based on a new batch. After reaching the last batch, you should begin again with the first. In other words, at the `k`'th step you should use the `(k % num_batches)`'th batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:45.510189Z",
     "iopub.status.busy": "2025-02-18T17:53:45.509863Z",
     "iopub.status.idle": "2025-02-18T17:53:45.515346Z",
     "shell.execute_reply": "2025-02-18T17:53:45.514919Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(GRAD,D,batch_size,mu_init,sigma2_init,num_epochs):\n",
    "\n",
    "    # Split D into batches (see item 1 above)\n",
    "    ...  # TODO\n",
    "\n",
    "    # Initialization (same as GD_2D)\n",
    "    gamma_mu, gamma_sigma2 = 1,20\n",
    "    numsteps = num_epochs*num_batches      # each epoch is a sweep though all of the batches in the dataset\n",
    "    mu = np.empty(numsteps)\n",
    "    mu[0] = mu_init\n",
    "    sigma2 = np.empty(numsteps)\n",
    "    sigma2[0] = sigma2_init\n",
    "\n",
    "    for k in range(numsteps-1):\n",
    "\n",
    "        # Choose the batch (see item 2 above)\n",
    "        B = ...   # TODO\n",
    "\n",
    "        # Same as GD_2D\n",
    "        delmu, delsigma2 = ...  # TODO\n",
    "        mu[k+1] = ...  # TODO\n",
    "        sigma2[k+1] = ...  # TODO\n",
    "        \n",
    "    return mu, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:45.516973Z",
     "iopub.status.busy": "2025-02-18T17:53:45.516808Z",
     "iopub.status.idle": "2025-02-18T17:53:45.520582Z",
     "shell.execute_reply": "2025-02-18T17:53:45.520207Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_gd, sigma2_gd = SGD(gradient,D,batch_size=5,mu_init=0.5,sigma2_init=10,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### SGD trajectories\n",
    "\n",
    "The next cell repeats the plots we made for gradient descent, but now using stochastic gradient descent. Notice that the trajectories are much more jagged, and they do not actually converge to the true solution. In this sense, stochastic gradient descent is *not* a good algorithm for solving optimization problems. However it is popular in machine learning applications because a) it is faster than GD for large datasets, and b) approximate convergence (as opposed to exact convergence) is often good enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:45.548995Z",
     "iopub.status.busy": "2025-02-18T17:53:45.548817Z",
     "iopub.status.idle": "2025-02-18T17:53:46.205655Z",
     "shell.execute_reply": "2025-02-18T17:53:46.205209Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_range = np.linspace(0.1,2,10)\n",
    "sigma2_range = np.arange(15,25)\n",
    "batch_size = 5\n",
    "\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('$\\\\mu_k$',fontsize=16)\n",
    "ax1.set_ylabel('$\\\\sigma^2_k$',fontsize=16)\n",
    "\n",
    "_, ax2 = plt.subplots(nrows=2,sharex=True)\n",
    "ax2[0].set_ylabel('$\\\\mu_k$',fontsize=16)\n",
    "ax2[1].set_ylabel('$\\\\sigma^2_k$',fontsize=16)\n",
    "ax2[1].set_xlabel('$k$',fontsize=16)\n",
    "\n",
    "for i, mu_init in enumerate(mu_range):\n",
    "    for j, sigma2_init in enumerate(sigma2_range):\n",
    "        mu_gd, sigma2_gd = SGD(gradient,D,batch_size,mu_init=mu_init,sigma2_init=sigma2_init,num_epochs=10)\n",
    "        ax1.plot(mu_gd,sigma2_gd,linewidth=0.5)\n",
    "        ax2[0].plot(mu_gd)\n",
    "        ax2[1].plot(sigma2_gd)\n",
    "\n",
    "ax1.plot(mu_star,sigma2_star,'r*',markersize=16)\n",
    "ax2[0].axhline(mu_star,0,100,color='r',linestyle='--')\n",
    "ax2[1].axhline(sigma2_star,0,100,color='r',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### The effect of batch size\n",
    "\n",
    "The plot below illustrates SGD with different batch sizes ranging from 1 (pure SGD) to 50 (pure gradient descent, since $N=50$). The plot shows the convergence of the algorithm to the solution, when starting from a common initial condition. The x-axis is the number of samples processed. A dot marker is placed at each SGD step. With a batch size of 50, fewer steps are taken, but each step is of high quality, leading to a smoother but slower convergence. With a batch size of 1 (pure SGD), approximate convergence to the solution is quick, but noisy, and the algorithm never settles down. We can see that a good approach might be to start with small batches, in order to get quick initial convergence, and then increase the batch size in order to zero in on the solution. Indeed, this is often done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-18T17:53:46.207313Z",
     "iopub.status.busy": "2025-02-18T17:53:46.207116Z",
     "iopub.status.idle": "2025-02-18T17:53:46.600935Z",
     "shell.execute_reply": "2025-02-18T17:53:46.600429Z"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8,8),nrows=2,sharex=True)\n",
    "ax[0].set_ylabel('$\\\\mu_k$',fontsize=16)\n",
    "ax[1].set_ylabel('$\\\\sigma^2_k$',fontsize=16)\n",
    "ax[1].set_xlabel('number of samples processed',fontsize=12)\n",
    "\n",
    "num_epochs=10\n",
    "batch_sizes = [1,5,10,50]\n",
    "\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    mu_gd, sigma2_gd = SGD(gradient,D,batch_size=batch_size,mu_init=2.0,sigma2_init=25,num_epochs=num_epochs)\n",
    "    x = np.arange(0,D.shape[0]*num_epochs,batch_size)\n",
    "    ax[0].plot(x,mu_gd,marker='.',label=f'batch size = {batch_size}')\n",
    "    ax[1].plot(x,sigma2_gd,marker='.',label=f'batch size = {batch_size}')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Make sure you submit the .zip file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "II_1": {
     "name": "II_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> np.isclose(curlyJ2(0, 1, D), 10.6557482, 0.001) is np.True_\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> get_hash([curlyJ2(mu, sigma2, D) for mu in range(4) for sigma2 in range(1, 4)], 4) == '71919c8db1455976001ea4bb9cae79b8'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_2_1": {
     "name": "II_2_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_star, sigma2_star = eval_mu_sigma2_star(D)\n>>> get_hash(mu_star, 4) == '9223d5a3ad20bbbaf763756df695cc2d' and get_hash(sigma2_star, 4) == '026c38605d82f069181eb3d675193143'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_star, sigma2_star = eval_mu_sigma2_star(D)\n>>> get_hash(mu_star, 4) == '9223d5a3ad20bbbaf763756df695cc2d' and get_hash(sigma2_star, 4) == '026c38605d82f069181eb3d675193143'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_3_1": {
     "name": "II_3_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> dmu, dsigma2 = gradient(0, 1, D)\n>>> np.isclose(dmu, -0.78966215, 4) and np.isclose(dsigma2, -9.2368097) is np.True_\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> dmu, dsigma2 = gradient(2, 2, D)\n>>> get_hash(dmu, 4) == '3146ba798f55281172c33f4990e79d0d' and get_hash(dsigma2, 4) == 'fc768ee140db53cf2442af9c8c54cba2'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_3_2": {
     "name": "II_3_2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = GD_2D(gradient, D, mu_init=0.5, sigma2_init=10, num_epochs=2)\n>>> mu_gd.shape == (100,) and sigma2_gd.shape == (100,) and np.isclose(mu_gd[0], 0.5, 0.01) and (np.isclose(sigma2_gd[0], 10.0, 0.01) is np.True_)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = GD_2D(gradient, D, mu_init=0.5, sigma2_init=10, num_epochs=2)\n>>> get_hash(mu_gd, 3) == 'b24d527406c641c05a3e8aa8e44b9568'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = GD_2D(gradient, D, mu_init=0.5, sigma2_init=10, num_epochs=2)\n>>> get_hash(sigma2_gd, 3) == 'dfe9d8e27f0ce719e5f4a8a9213aeb30'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_4_1": {
     "name": "II_4_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = SGD(gradient, D, batch_size=5, mu_init=0.5, sigma2_init=10, num_epochs=10)\n>>> mu_gd.shape == (100,) and sigma2_gd.shape == (100,)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = SGD(gradient, D, batch_size=5, mu_init=0.5, sigma2_init=10, num_epochs=10)\n>>> np.all(np.isclose(mu_gd[:2], [0.5, 0.39770129], 0.001)) and np.all(np.isclose(sigma2_gd[:2], [10.0, 9.77642568], 0.001)) is np.True_\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = SGD(gradient, D, batch_size=5, mu_init=0.5, sigma2_init=10, num_epochs=10)\n>>> get_hash(mu_gd, 4) == '61c27921e9f8ec07eb86e3625c273726'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> D = draw_50_samples_from_II()\n>>> mu_gd, sigma2_gd = SGD(gradient, D, batch_size=5, mu_init=0.5, sigma2_init=10, num_epochs=10)\n>>> get_hash(sigma2_gd, 4) == '941128bc10734f44a03e35643930e427'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_2": {
     "name": "I_2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> np.isclose(curlyJ1(1, D), 7.126381758478155, 0.01) is np.True_\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> get_hash(curlyJ1(10, D), 4) == 'eca8bb6fcb9c6f3fc5c3c66c23fd61ad'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_3_1": {
     "name": "I_3_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> get_hash(eval_lambda_star(D), 4) == '71e5b18ccdaf2704464b49c77333e37e'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_3_3_1": {
     "name": "I_3_3_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(J1_samples[0, 0], 4.703676266804346, 0.01) and J1_samples.shape == (300, 50)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> get_hash(J1_samples, 3) == '36e455ee9a1f943ed5bea75d684be130'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_4_1": {
     "name": "I_4_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> np.isclose(curlyJ1prime(1, D), 6.126381758478155, 0.01) is np.True_\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> get_hash([curlyJ1prime(x, D) for x in range(1, 10)], 5) == 'ef43a09a89cf8481f1b1df2bc44acfc7'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_4_3": {
     "name": "I_4_3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> lmbdas_gd = GD_1D(curlyJ1prime, D)\n>>> np.isclose(lmbdas_gd[0], 0.3, 0.001) is np.True_\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> lmbdas_gd = GD_1D(curlyJ1prime, D)\n>>> get_hash(lmbdas_gd, 4) == '7287a691f84e3da92331a32a7ff9e64b'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_4_5": {
     "name": "I_4_5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> D = draw_50_samples_from_I()\n>>> lmbdas_gd = GD_1D(curlyJ1prime, D)\n>>> get_hash(count_J1prime_evals(lmbdas_gd, eval_lambda_star(D)), 2) == '01deebfa87b938c89ecb3dcb4effcbe2'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
